# Vanishing & Exploding Gradients with Activations and BatchNorm

## 1. Vanishing and Exploding Gradient Problem

In deep neural networks, during **backpropagation**, gradients are calculated as a chain of derivatives:

\[ \frac{\partial L}{\partial W} = \prod_{i=1}^{n} \frac{\partial a^{[i]}}{\partial z^{[i]}} \cdot \frac{\partial z^{[i]}}{\partial W^{[i]}} \]

If these derivatives are:
- **< 1**: Gradients shrink exponentially \u2192 **vanishing gradients**
- **> 1**: Gradients grow exponentially \u2192 **exploding gradients**

### Consequences
- Vanishing: Early layers receive little to no gradient, slow or no learning
- Exploding: Weights become unstable, leading to NaNs or diverging loss

---

## 2. Activation Functions and Their Derivatives

### a. Sigmoid
\[ \sigma(x) = \frac{1}{1 + e^{-x}}, \quad \sigma'(x) = \sigma(x)(1 - \sigma(x)) \]

**Pros:** Smooth, bounded between (0, 1)  
**Cons:**
- Derivative \u2264 0.25
- Saturates for large |x| \u2192 gradients \u2248 0 \u2192 **vanishing**

---

### b. Tanh
\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}, \quad \text{derivative: } 1 - \tanh^2(x) \]

**Pros:** Centered around 0  
**Cons:**
- Saturates at -1 or 1 for large |x| \u2192 derivative \u2248 0 \u2192 **vanishing**

---

### c. ReLU (Rectified Linear Unit)
\[ f(x) = \max(0, x), \quad f'(x) = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases} \]

**Pros:**
- Non-saturating for positive values
- Derivative = 1 (for x > 0) \u2192 helps gradient flow

**Cons:**
- Derivative = 0 (for x \u2264 0) \u2192 **dead neurons**

---

## 3. Batch Normalization (BatchNorm)

### What It Does
For each mini-batch, it:
1. **Normalizes activations**: mean = 0, variance = 1
\[ \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \]
2. **Applies learnable shift and scale**:
\[ y_i = \gamma \hat{x}_i + \beta \]

---

### How It Helps

#### Prevents Vanishing Gradients
- Keeps inputs to activation functions in a **non-saturated range**
- Preserves gradient flow during backprop

#### Prevents Exploding Gradients
- Keeps activations from growing exponentially
- Maintains stable variance across layers

#### Improves Gradient Flow
- Enables effective learning in very deep networks

---

## 4. Summary Table

| Activation | Derivative Behavior | Gradient Issue | BatchNorm Helps? |
|------------|----------------------|----------------|------------------|
| Sigmoid    | ~0 to 0.25           | Vanishes fast  | \u2705 Yes        |
| Tanh       | ~0 to 1              | Still vanishes | \u2705 Yes        |
| ReLU       | 0 or 1               | May die        | \u2705 Yes        |

---

## 5. Final Intuition

> BatchNorm acts as a **stabilizer** for activations and gradients. By normalizing intermediate activations, it ensures that derivatives stay in a good range, helping both forward activations and backward gradients stay well-behaved. This counters the core of the vanishing/exploding gradient problem.

---

## References for Interview Prep
- Gradient flow math: Chain rule of backprop
- Activation ranges and saturation behavior
- ReLU's non-saturation property
- Role of normalization in deep learning
- Empirical benefits in training deep CNNs and transformers

